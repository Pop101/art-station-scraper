{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def files_to_df(dir_name):\n",
    "    \n",
    "    full_df = pd.DataFrame() # initialize full df compilation of files\n",
    "    files = os.listdir(dir_name) # get list of files in directory\n",
    "    print(f\"{len(files)} files found in {dir_name}\")\n",
    "\n",
    "    for i in files:\n",
    "        filename = os.path.join(dir_name, i)\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.loads(json.load(f))['data']\n",
    "            df = pd.DataFrame(data)\n",
    "        full_df = pd.concat([full_df, df], ignore_index = True) # join current json file to full df\n",
    "\n",
    "    return full_df\n",
    "\n",
    "\n",
    "def df_to_json(query):\n",
    "    \n",
    "    print(f\"Converting json files in raw_data/{query} to dataframe.\")\n",
    "    df = files_to_df(f'raw_data/{query}')\n",
    "    print(f\"Shape of dataframe: {df.shape}\")\n",
    "        \n",
    "    # convert dataframe back to json and export it\n",
    "    export = df.to_json(f\"full_data/{query}.json\")\n",
    "    print(\"Export complete.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "def pause_report(length, random_delay, file_count, disp = True):\n",
    "    \n",
    "    pause_time = length*np.random.normal(1,random_delay)\n",
    "    if disp: print(f\"Downloaded {file_count} images. Pausing scraper for {round(pause_time,2)} seconds.\")\n",
    "    time.sleep(pause_time)\n",
    "\n",
    "    \n",
    "def download_files(query, df, file_start, file_end, short_pause = 2, long_pause = 60, random_delay = 0.25):\n",
    "\n",
    "    file_count = 0 #number of files downloaded\n",
    "    alrdy_exists = 0 #number of files that already exist and are skipped over\n",
    "    skipped_urls = {} #urls that can't be connected to - need to retry at a later time\n",
    "    \n",
    "    if not os.path.exists(f'images/{query}'):\n",
    "        print(f'Creating directory: images/{query}')\n",
    "        os.makedirs(f'images/{query}')\n",
    "    \n",
    "    for i in range(file_start, file_end):\n",
    "\n",
    "        if i >= len(df):\n",
    "            print(f\"Index {i} does not exist in dataframe.\")\n",
    "            print(\"End of dataframe has probably been reached. Process is terminating.\")\n",
    "            return 0\n",
    "        \n",
    "        filename = f\"images/{query}/{df['hash_id'][i]}.jpg\" # name the image using its hash_id\n",
    "\n",
    "        if not os.path.exists(filename): # skip over files that have been downloaded\n",
    "            \n",
    "            try:\n",
    "                img_url = df['smaller_square_cover_url'][i]\n",
    "                print(f'Downloading {img_url} as {filename}')\n",
    "                \n",
    "                f = open(filename,'wb')\n",
    "                f.write(requests.get(img_url).content)\n",
    "                f.close()\n",
    "\n",
    "                file_count += 1\n",
    "\n",
    "                # take a short pause per url scraped\n",
    "                pause_report(short_pause, random_delay, file_count)\n",
    "                \n",
    "            except:\n",
    "                print(\"Cannot establish connection with the following row in df:\")\n",
    "                print(f\"{i}:{img_url}\")\n",
    "                print(\"Skipping to next row\")\n",
    "                skipped_urls[i] = img_url #add key value pair to skipped_urls dict\n",
    "                \n",
    "                if len(skipped_urls) > 10:\n",
    "                    print(\"More than 10 urls have had connection error.\")\n",
    "                    print(skipped_urls)\n",
    "                    pause_report(300, random_delay, file_count) # pause for 5 minutes if more than 10 urls with connection error\n",
    "                       \n",
    "        else: \n",
    "            print(f\"{filename} already exists. Skipping to next image url in df.\")\n",
    "            alrdy_exists += 1\n",
    "            \n",
    "        # take a long pause if 100 images have been downloaded\n",
    "        if file_count > 0 and file_count%100 == 0:\n",
    "            pause_report(long_pause, random_delay, file_count)\n",
    "            \n",
    "        # take a long pause x 2 if 500 images have been downloaded\n",
    "        if file_count > 0 and file_count%500 == 0:\n",
    "            pause_report(long_pause*2, random_delay, file_count)\n",
    "      \n",
    "    # Retry skipped urls\n",
    "    print(f\"Skipped the following urls because failed to make a connection:\")\n",
    "    print(skipped_urls)\n",
    "    \n",
    "    if len(skipped_urls) > 0:\n",
    "        print(\"Retrying skipped urls.\")\n",
    "        \n",
    "        for index, url in skipped_urls.items():\n",
    "            try:\n",
    "                filename = f\"images/{query}/{df['hash_id'][index]}.jpg\"\n",
    "                print(f'Downloading {url} as {filename}.')\n",
    "\n",
    "                f = open(filename,'wb')\n",
    "                f.write(requests.get(img_url).content)\n",
    "                f.close()\n",
    "\n",
    "                file_count += 1\n",
    "\n",
    "                # take a short pause per url scraped\n",
    "                pause_report(short_pause, random_delay, file_count)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Still unable to connect to skipped urls. Please manually check for error.\")\n",
    "                print(e.message, e.args)\n",
    "                return 0\n",
    "    \n",
    "    print(f\"{file_count} images downloaded.\")\n",
    "    print(f\"Skipped {alrdy_exists} images because already exists in database.\")\n",
    "    print(\"Process finished.\")\n",
    "    return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['cyberpunk','noir','horror','western','cartoon','steampunk']\n",
    "\n",
    "# convert the raw json files of a specific keyword to a dataframe\n",
    "df = df_to_json(genres[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the image download process\n",
    "download_files(genres[2], df, 0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "plt.figure(figsize=(20,6))\n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    img = mpimg.imread(f\"images/{query}/{df['hash_id'][i]}.jpg\")\n",
    "    plt.title(f\"{df['title'][i]}\")\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
